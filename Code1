!apt-get update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz
!tar xf spark-2.3.1-bin-hadoop2.7.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.3.1-bin-hadoop2.7"

!ls

import findspark
findspark.init()

import pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate() 
spark




from pyspark.sql.types import *
schema = StructType([
  StructField('ip_address', StringType()),
  StructField('Country', StringType()),
  StructField('domain_name', StringType()),
  StructField('bytes_used', IntegerType()),
])

df = spark.read.csv("challenge.csv", header=True, schema=schema)
df.show()


from pyspark.sql.functions import *
df = df.withColumn('mexico', when(df.Country == 'Mexico', 'Yes').otherwise('no'))
df.show()


import pyspark.sql.functions as sqlfunc
df1 = df.groupBy('mexico').agg(sqlfunc.sum('bytes_used'))
df1.show()


import pyspark.sql.functions as sqlfunc
df1 = df.groupBy('Country').agg(sqlfunc.countDistinct('ip_address').alias('sum of ips'))
df1.show()
